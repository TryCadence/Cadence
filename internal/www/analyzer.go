package www

import (
	"fmt"
	"regexp"
	"strings"
	"unicode"
)

// Package www provides website and code content analysis for detecting
// AI-generated content and "slop" patterns in code and documentation.

// ContentAnalysis represents analysis results for code/content
type ContentAnalysis struct {
	TotalLines            int
	TotalTokens           int
	UniqueWords           int
	AverageWordLength     float64
	AverageLineLength     float64
	CommentDensity        float64 // 0.0-1.0: ratio of comment lines to code lines
	SuspiciousnessScore   float64 // 0.0-1.0: overall likelihood of AI generation
	PatternMatches        []*PatternMatch
	NamingConsistency     float64 // 0.0-1.0: how consistent naming conventions are
	FormattingConsistency float64 // 0.0-1.0: how consistent code formatting is
	StructuralConsistency float64 // 0.0-1.0: how consistent function/class structures are
	AILikelihoodFactors   map[string]float64
	HasSuspiciousPatterns bool
	HasExcessiveComments  bool
	HasPerfectFormatting  bool
	HasRepetitivePatterns bool
	HasNamingAnomaly      bool
}

// PatternType represents different types of suspicious patterns
type PatternType string

const (
	PatternExcessiveComments    PatternType = "excessive_comments"
	PatternUnusualNaming        PatternType = "unusual_naming"
	PatternSyntacticConsistency PatternType = "syntactic_consistency"
	PatternTokenRepetition      PatternType = "token_repetition"
	PatternFunctionStructure    PatternType = "function_structure"
	PatternExcessiveEmoji       PatternType = "excessive_emoji"
	PatternPerfectFormatting    PatternType = "perfect_formatting"
	PatternBoilerplate          PatternType = "boilerplate_code"
	PatternAutogeneratedMarkers PatternType = "autogenerated_markers"
	PatternOverExplanation      PatternType = "over_explanation"
)

// PatternMatch represents a detected suspicious pattern in code/content
type PatternMatch struct {
	Type        PatternType
	Severity    float64 // 0.0-1.0
	Description string
	Location    string
	Evidence    string
}

// Analyzer provides code content analysis capabilities
type Analyzer struct {
	strictMode bool
}

// New creates a new Analyzer instance
func New(strictMode bool) *Analyzer {
	return &Analyzer{
		strictMode: strictMode,
	}
}

// Analyze performs comprehensive content analysis
func (a *Analyzer) Analyze(content string) *ContentAnalysis {
	if content == "" {
		return &ContentAnalysis{
			SuspiciousnessScore: 0,
			PatternMatches:      make([]*PatternMatch, 0),
			AILikelihoodFactors: make(map[string]float64),
		}
	}

	analysis := &ContentAnalysis{
		AILikelihoodFactors: make(map[string]float64),
		PatternMatches:      make([]*PatternMatch, 0),
	}

	lines := strings.Split(content, "\n")
	analysis.TotalLines = len(lines)

	// Analyze structure
	analysis.AverageLineLength = a.calculateAverageLineLength(lines)
	analysis.CommentDensity = a.calculateCommentDensity(lines)
	analysis.TotalTokens = a.countTokens(content)
	analysis.UniqueWords = a.countUniqueWords(content)
	analysis.AverageWordLength = a.calculateAverageWordLength(content)

	// Detect patterns
	patterns := a.DetectPatterns(content)
	analysis.PatternMatches = patterns

	// Calculate consistency metrics
	analysis.NamingConsistency = a.calculateNamingConsistency(content)
	analysis.FormattingConsistency = a.calculateFormattingConsistency(lines)
	analysis.StructuralConsistency = a.calculateStructuralConsistency(lines)

	// Set flags
	for _, pattern := range patterns {
		switch pattern.Type {
		case PatternExcessiveComments:
			analysis.HasExcessiveComments = true
		case PatternPerfectFormatting:
			analysis.HasPerfectFormatting = true
		case PatternTokenRepetition:
			analysis.HasRepetitivePatterns = true
		case PatternUnusualNaming:
			analysis.HasNamingAnomaly = true
		}
	}

	analysis.HasSuspiciousPatterns = len(patterns) > 0

	// Calculate overall suspiciousness score
	analysis.SuspiciousnessScore = a.calculateSuspiciousnessScore(analysis)
	analysis.AILikelihoodFactors = a.calculateLikelihoodFactors(analysis)

	return analysis
}

// DetectPatterns identifies suspicious patterns in code
func (a *Analyzer) DetectPatterns(content string) []*PatternMatch {
	patterns := make([]*PatternMatch, 0)

	// Check for excessive comments
	if match := a.detectExcessiveComments(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for excessive emoji usage
	if match := a.detectExcessiveEmoji(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for autogenerated markers
	if match := a.detectAutogeneratedMarkers(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for over-explanation patterns
	if match := a.detectOverExplanation(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for boilerplate code patterns
	if match := a.detectBoilerplate(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for perfect formatting
	if match := a.detectPerfectFormatting(content); match != nil {
		patterns = append(patterns, match)
	}

	// Check for repetitive patterns
	if matches := a.detectTokenRepetition(content); len(matches) > 0 {
		patterns = append(patterns, matches...)
	}

	return patterns
}

// detectExcessiveComments identifies content with too many explanatory comments
func (a *Analyzer) detectExcessiveComments(content string) *PatternMatch {
	lines := strings.Split(content, "\n")
	commentCount := 0
	codeLineCount := 0

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") {
			commentCount++
		} else if trimmed != "" && !strings.HasPrefix(trimmed, "*") {
			codeLineCount++
		}
	}

	if codeLineCount > 0 {
		ratio := float64(commentCount) / float64(codeLineCount+commentCount)
		threshold := 0.35 // >35% comments is suspicious
		if ratio > threshold {
			return &PatternMatch{
				Type:        PatternExcessiveComments,
				Severity:    (ratio - threshold) / (1.0 - threshold),
				Description: "Excessive comment density - typical of AI-generated code",
				Evidence:    "Comments: " + floatToPercent(ratio),
				Location:    "Global",
			}
		}
	}
	return nil
}

// detectExcessiveEmoji identifies content with too many emojis
func (a *Analyzer) detectExcessiveEmoji(content string) *PatternMatch {
	emojiCount := 0
	emojiRanges := []struct {
		start rune
		end   rune
	}{
		{0x1F300, 0x1F9FF}, // Emoticons, Symbols, and Pictographs
		{0x2600, 0x27BF},   // Miscellaneous Symbols
		{0x1F900, 0x1F9FF}, // Supplemental Symbols and Pictographs
	}

	for _, r := range content {
		for _, rang := range emojiRanges {
			if r >= rang.start && r <= rang.end {
				emojiCount++
				break
			}
		}
	}

	if emojiCount > 5 { // More than a few emojis
		severity := float64(emojiCount) / float64(len(content)/10)
		if severity > 1.0 {
			severity = 1.0
		}
		return &PatternMatch{
			Type:        PatternExcessiveEmoji,
			Severity:    severity,
			Description: "Excessive emoji usage - common in AI-generated marketing/doc content",
			Evidence:    "Found " + intToString(emojiCount) + " emojis",
			Location:    "Global",
		}
	}
	return nil
}

// detectAutogeneratedMarkers identifies common AI generation markers
func (a *Analyzer) detectAutogeneratedMarkers(content string) *PatternMatch {
	markers := []string{
		"auto-generated", "automatically generated", "generated by",
		"do not edit", "do not modify", "generated file",
		"this file was generated",
	}

	lowerContent := strings.ToLower(content)
	for _, marker := range markers {
		if strings.Contains(lowerContent, marker) {
			return &PatternMatch{
				Type:        PatternAutogeneratedMarkers,
				Severity:    0.8,
				Description: "Contains autogeneration markers or warnings",
				Evidence:    "Found: '" + marker + "'",
				Location:    "Header/Comments",
			}
		}
	}
	return nil
}

// detectOverExplanation identifies overly verbose explanations
func (a *Analyzer) detectOverExplanation(content string) *PatternMatch {
	lines := strings.Split(content, "\n")
	for i := 0; i < len(lines)-1; i++ {
		if strings.Contains(strings.ToLower(lines[i]), "//") {
			trimmed := strings.TrimSpace(lines[i])
			// Check if comment is significantly longer than the code it describes
			if i+1 < len(lines) && len(trimmed) > 50 {
				codeLine := strings.TrimSpace(lines[i+1])
				if len(codeLine) < len(trimmed)/3 && codeLine != "" {
					return &PatternMatch{
						Type:        PatternOverExplanation,
						Severity:    0.6,
						Description: "Over-explanation of code - verbose comments for simple logic",
						Evidence:    "Comment length disproportionate to code",
						Location:    "Line ~" + intToString(i+1),
					}
				}
			}
		}
	}
	return nil
}

// detectBoilerplate identifies excessive boilerplate code
func (a *Analyzer) detectBoilerplate(content string) *PatternMatch {
	boilerplatePatterns := []string{
		"TODO", "FIXME", "HACK", "XXX", "DEPRECATED",
		"function main", "public static void main", "def main",
	}

	count := 0
	for _, pattern := range boilerplatePatterns {
		count += strings.Count(content, pattern)
	}

	if count > 5 {
		return &PatternMatch{
			Type:        PatternBoilerplate,
			Severity:    0.5,
			Description: "Excessive boilerplate code patterns - common in generated templates",
			Evidence:    "Found " + intToString(count) + " boilerplate markers",
			Location:    "Global",
		}
	}
	return nil
}

// detectPerfectFormatting identifies suspiciously perfect code formatting
func (a *Analyzer) detectPerfectFormatting(content string) *PatternMatch {
	lines := strings.Split(content, "\n")
	indentLengths := make(map[int]int)

	for _, line := range lines {
		if len(line) > 0 && line[0] == ' ' || line[0] == '\t' {
			indent := len(line) - len(strings.TrimLeft(line, " \t"))
			indentLengths[indent]++
		}
	}

	// Check if indentation is perfectly consistent
	if len(indentLengths) <= 2 && len(lines) > 10 {
		return &PatternMatch{
			Type:        PatternPerfectFormatting,
			Severity:    0.4,
			Description: "Suspiciously perfect and consistent code formatting",
			Evidence:    "Only " + intToString(len(indentLengths)) + " indent levels found",
			Location:    "Global",
		}
	}
	return nil
}

// detectTokenRepetition identifies repetitive token patterns
func (a *Analyzer) detectTokenRepetition(content string) []*PatternMatch {
	patterns := make([]*PatternMatch, 0)
	tokens := a.extractTokens(content)
	tokenFreq := make(map[string]int)

	for _, token := range tokens {
		tokenFreq[token]++
	}

	// Check for excessive token repetition
	for token, freq := range tokenFreq {
		if freq > 20 && len(token) > 5 {
			ratio := float64(freq) / float64(len(tokens))
			if ratio > 0.05 { // Token appears >5% of time
				patterns = append(patterns, &PatternMatch{
					Type:        PatternTokenRepetition,
					Severity:    ratio,
					Description: "Excessive token repetition - sign of generated code",
					Evidence:    "Token '" + token + "' appears " + intToString(freq) + " times",
					Location:    "Global",
				})
			}
		}
	}

	return patterns
}

// Helper functions

func (a *Analyzer) calculateAverageLineLength(lines []string) float64 {
	if len(lines) == 0 {
		return 0
	}
	totalLen := 0
	for _, line := range lines {
		totalLen += len(line)
	}
	return float64(totalLen) / float64(len(lines))
}

func (a *Analyzer) calculateCommentDensity(lines []string) float64 {
	if len(lines) == 0 {
		return 0
	}
	commentLines := 0
	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") || strings.HasPrefix(trimmed, "*") {
			commentLines++
		}
	}
	return float64(commentLines) / float64(len(lines))
}

func (a *Analyzer) countTokens(content string) int {
	fields := strings.Fields(content)
	return len(fields)
}

func (a *Analyzer) countUniqueWords(content string) int {
	words := make(map[string]bool)
	fields := strings.Fields(strings.ToLower(content))
	for _, word := range fields {
		clean := strings.Trim(word, ".,;:!?()[]{}\"'")
		if clean != "" && !isSpecialCharacter(clean) {
			words[clean] = true
		}
	}
	return len(words)
}

func (a *Analyzer) calculateAverageWordLength(content string) float64 {
	fields := strings.Fields(content)
	if len(fields) == 0 {
		return 0
	}
	totalLen := 0
	for _, word := range fields {
		totalLen += len(word)
	}
	return float64(totalLen) / float64(len(fields))
}

func (a *Analyzer) calculateNamingConsistency(content string) float64 {
	// Simple heuristic: check if variable/function names follow consistent patterns
	// e.g., snake_case vs camelCase
	snakeCaseCount := strings.Count(content, "_")
	camelCasePattern := regexp.MustCompile(`[a-z][A-Z]`)
	camelCaseCount := len(camelCasePattern.FindAllString(content, -1))

	if snakeCaseCount == 0 && camelCaseCount == 0 {
		return 1.0 // Consistent (all single words)
	}

	total := snakeCaseCount + camelCaseCount
	if total == 0 {
		return 1.0
	}

	// Calculate how skewed to one style
	ratio := float64(snakeCaseCount) / float64(total)
	if ratio > 0.8 || ratio < 0.2 {
		return 0.9 // Highly consistent
	}
	return 0.5 // Mixed styles
}

func (a *Analyzer) calculateFormattingConsistency(lines []string) float64 {
	if len(lines) < 5 {
		return 0.5
	}

	indents := make(map[int]int)
	for _, line := range lines {
		if len(line) > 0 {
			spaces := len(line) - len(strings.TrimLeft(line, " \t"))
			indents[spaces]++
		}
	}

	if len(indents) <= 2 {
		return 0.95 // Very consistent
	} else if len(indents) <= 4 {
		return 0.75
	}
	return 0.5
}

func (a *Analyzer) calculateStructuralConsistency(lines []string) float64 {
	// Check for similar function/method structure lengths
	funcPattern := regexp.MustCompile(`(func |def |function |const |class )`)
	matches := funcPattern.FindAllString(strings.Join(lines, "\n"), -1)
	if len(matches) < 2 {
		return 0.5
	}
	return 0.7 // Heuristic for now
}

func (a *Analyzer) calculateSuspiciousnessScore(analysis *ContentAnalysis) float64 {
	score := 0.0
	weights := 0.0

	// Weight each pattern
	for _, pattern := range analysis.PatternMatches {
		score += pattern.Severity * 0.2
		weights += 0.2
	}

	// Weight comment density
	if analysis.CommentDensity > 0.3 {
		score += (analysis.CommentDensity - 0.3) * 0.15
		weights += 0.15
	}

	// Weight formatting consistency (too perfect is suspicious)
	if analysis.FormattingConsistency > 0.9 {
		score += (analysis.FormattingConsistency - 0.9) * 0.1
		weights += 0.1
	}

	if weights > 0 {
		return score / weights
	}
	return score
}

func (a *Analyzer) calculateLikelihoodFactors(analysis *ContentAnalysis) map[string]float64 {
	factors := make(map[string]float64)
	factors["comment_density"] = analysis.CommentDensity
	factors["formatting_consistency"] = analysis.FormattingConsistency
	factors["naming_consistency"] = analysis.NamingConsistency
	factors["pattern_count"] = float64(len(analysis.PatternMatches)) / 10.0
	if factors["pattern_count"] > 1.0 {
		factors["pattern_count"] = 1.0
	}
	return factors
}

func (a *Analyzer) extractTokens(content string) []string {
	tokenRegex := regexp.MustCompile(`\b\w+\b`)
	return tokenRegex.FindAllString(content, -1)
}

// Utility functions

func isSpecialCharacter(s string) bool {
	for _, r := range s {
		if !unicode.IsLetter(r) && !unicode.IsNumber(r) && r != '_' {
			return true
		}
	}
	return false
}

func floatToPercent(f float64) string {
	percent := int(f * 100)
	return intToString(percent) + "%"
}

func intToString(i int) string {
	if i < 10 {
		return string(rune('0' + i))
	}
	return fmt.Sprintf("%d", i)
}
